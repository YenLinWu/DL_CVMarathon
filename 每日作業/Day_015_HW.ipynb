{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習內容』\n",
    "#### 運用這幾天所學觀念搭建一個CNN分類器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習目的』\n",
    "  #### 熟悉CNN分類器搭建步驟與原理\n",
    "  #### 學員們可以嘗試不同搭法，如使用不同的Maxpooling層，用GlobalAveragePooling取代Flatten等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料   \n",
    "\n",
    "[CIFAR10/CIFAR100 數據集介紹](https://www.itread01.com/content/1519212167.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data = (50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "( x_train, y_train ), ( x_test, y_test ) = cifar10.load_data()\n",
    "\n",
    "print( f'Shape of Training Data = {x_train.shape}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize Data\n",
    "def normalize( X_train, X_test ):\n",
    "    mean = np.mean( X_train, axis=(0,1,2,3) )\n",
    "    std = np.std( X_train, axis=(0,1,2,3) )\n",
    "    X_train = (X_train-mean)/(std+1e-7)\n",
    "    X_test = (X_test-mean)/(std+1e-7) \n",
    "    return X_train, X_test, mean, std\n",
    "    \n",
    "## Normalize Training and Testset    \n",
    "x_train, x_test, mean_train, std_train = normalize( x_train, x_test ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 獨熱編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "OneHot Label 由 ( None, 1 ) -> ( None, 10 )\n",
    "例如 label=2 轉變成 [0,0,1,0,0,0,0,0,0,0]\n",
    "'''\n",
    "one_hot = OneHotEncoder()\n",
    "y_train = one_hot.fit_transform( y_train ).toarray()\n",
    "y_test = one_hot.transform( y_test ).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 CNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Convolution_1 (Conv2D)       (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "BN_1 (BatchNormalization)    (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "Max_Pooling (MaxPooling2D)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "Convolution_2 (Conv2D)       (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "BN_2 (BatchNormalization)    (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "FC (Dense)                   (None, 100)               1638500   \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 1,659,286\n",
      "Trainable params: 1,659,094\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential( name='CNN_Model' )\n",
    "\n",
    "# 卷積組合\n",
    "classifier.add( Conv2D( 32, kernel_size=(3,3), padding='same', input_shape=(32,32,3), activation='relu', name='Convolution_1' ) )\n",
    "classifier.add( BatchNormalization( name='BN_1' ) )\n",
    "\n",
    "# 池化\n",
    "classifier.add( MaxPooling2D( pool_size=(2,2), name='Max_Pooling' ) )\n",
    "\n",
    "# 卷積組合\n",
    "classifier.add( Conv2D( 64, kernel_size=(3,3), padding='same', activation='relu', name='Convolution_2' ) )\n",
    "classifier.add( BatchNormalization( name='BN_2' ) )\n",
    "\n",
    "# Flatten\n",
    "classifier.add( Flatten( name='Flatten' ) )\n",
    "\n",
    "# Fully Connected\n",
    "classifier.add( Dense( 100, activation='relu', name='FC' )) \n",
    "\n",
    "# 輸出\n",
    "classifier.add( Dense( 10, activation='softmax', name='Softmax' ) )\n",
    "\n",
    "classifier.summary( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 1.3260 - accuracy: 0.5389\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.8190 - accuracy: 0.7128\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.5678 - accuracy: 0.8029\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.3718 - accuracy: 0.8716\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.2302 - accuracy: 0.9219\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.1526 - accuracy: 0.9493\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.1267 - accuracy: 0.9561\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 3s 70us/sample - loss: 0.1117 - accuracy: 0.9619\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.1019 - accuracy: 0.9648\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 3s 66us/sample - loss: 0.0926 - accuracy: 0.9683s - loss: 0\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0698 - accuracy: 0.9756\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.0585 - accuracy: 0.9805\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0689 - accuracy: 0.9772\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.0711 - accuracy: 0.9760\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0612 - accuracy: 0.9805\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 4s 70us/sample - loss: 0.0463 - accuracy: 0.9839\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0526 - accuracy: 0.9824\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0465 - accuracy: 0.9843\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0478 - accuracy: 0.9841\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0503 - accuracy: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16a6f65ddc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#超過兩個就要選categorical_crossentrophy\n",
    "classifier.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'] )\n",
    "classifier.fit( x_train, y_train, batch_size=100, epochs=20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測新圖片，輸入影像前處理要與訓練時相同\n",
    "#### ((X-mean)/(std+1e-7) ):這裡的mean跟std是訓練集的\n",
    "## 維度如下方示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.6832943e-04, 7.9826656e-10, 2.4272746e-03, 9.9658352e-01,\n",
       "        1.4162015e-07, 8.0282987e-08, 1.6436826e-09, 2.3032469e-07,\n",
       "        2.0442099e-05, 7.3169866e-13]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example=( np.zeros(shape=(1,32,32,3))-mean_train )/( std_train+1e-7 ) \n",
    "classifier.predict( input_example )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
